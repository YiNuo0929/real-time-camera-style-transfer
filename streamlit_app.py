import streamlit as st
import onnxruntime as ort
import numpy as np
import cv2
import time
from io import BytesIO
from PIL import Image

# Streamlit é é¢è¨­å®š
st.set_page_config(page_title="ğŸ¨ Real-time Style Transfer (ONNX)", layout="wide")
st.title("ğŸ¥ Real-time Neural Style Transfer - ONNXRuntime GPU è‡ªå‹•åˆ‡æ›ç‰ˆ")
st.markdown("Upload a style image and activate your webcam to apply artistic style in real time!")

# âœ… å˜—è©¦å»ºç«‹ GPU Sessionï¼Œè‹¥å¤±æ•—å‰‡ fallback åˆ° CPU
@st.cache_resource
def load_onnx_model():
    try:
        session = ort.InferenceSession("stylization.onnx", providers=[
            "TensorrtExecutionProvider",
            "CUDAExecutionProvider",
            "CPUExecutionProvider"
        ])
    except Exception as e:
        st.warning(f"âš ï¸ GPU/TensorRT unavailable, fallback to CPU. Error: {e}")
        session = ort.InferenceSession("stylization.onnx", providers=["CPUExecutionProvider"])
    return session

# ä¸Šå‚³é¢¨æ ¼åœ–åƒ
style_image_file = st.file_uploader("Upload Style Image", type=["jpg", "jpeg", "png"])

if style_image_file:
    # è™•ç† style image
    image_data = style_image_file.read()
    image = Image.open(BytesIO(image_data)).convert('RGB')
    style_image = np.array(image).astype(np.float32)[np.newaxis, ...] / 255.
    style_image = cv2.resize(style_image[0], (256, 256))
    style_image = style_image[np.newaxis, ...].astype(np.float32)

    # è¼‰å…¥ ONNX æ¨¡å‹
    with st.spinner("Loading ONNX model..."):
        session = load_onnx_model()
        input_names = [i.name for i in session.get_inputs()]
        backend = session.get_providers()[0]
        st.success("âœ… ONNX æ¨¡å‹å·²æˆåŠŸè¼‰å…¥ï¼")
        st.markdown(f"ğŸ“¥ **æ¨¡å‹è¼¸å…¥åç¨±ï¼š** `{input_names}`")
        st.markdown(f"ğŸ§  **ä½¿ç”¨ä¸­çš„æ¨è«–å¾Œç«¯ï¼š** `{backend}`")

    if st.button("ğŸ¬ Start Stylization"):
        video_capture = cv2.VideoCapture(0)
        if not video_capture.isOpened():
            st.error("âŒ Cannot access webcam.")
        else:
            frame_display = st.empty()
            fps_display = st.empty()
            st.info("ğŸš¨ Press 'Stop' or close the app window to end.")

            prev_time = time.time()
            frame_count = 0

            while video_capture.isOpened():
                ret, frame = video_capture.read()
                if not ret:
                    break

                frame_count += 1

                # FPS è¨ˆç®—
                current_time = time.time()
                elapsed = current_time - prev_time
                if elapsed >= 1.0:
                    fps = frame_count / elapsed
                    fps_display.markdown(f"### ğŸŒ€ FPS: `{fps:.2f}`")
                    frame_count = 0
                    prev_time = current_time

                # é è™•ç†å…§å®¹åœ–åƒ
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                content_tensor = cv2.resize(frame_rgb, (256, 256))
                content_tensor = content_tensor.astype(np.float32)[np.newaxis, ...] / 255.

                # ONNX æ¨è«–
                result = session.run(None, {
                    input_names[0]: content_tensor,
                    input_names[1]: style_image
                })

                stylized_frame = (result[0][0] * 255).astype(np.uint8)
                frame_display.image(stylized_frame, channels="RGB", width=512)

            video_capture.release()